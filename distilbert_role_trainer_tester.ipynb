{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score,plot_confusion_matrix,multilabel_confusion_matrix\n",
    "import dill\n",
    "from transformers import EarlyStoppingCallback\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (0.3.3)\n",
      "Requirement already satisfied: imbalanced-learn in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from imbalanced-learn) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from imbalanced-learn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from imbalanced-learn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n",
      "Requirement already satisfied: TensorBoard in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (2.5.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (2.25.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (0.12.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (49.6.0.post20210108)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (3.15.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (0.36.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (1.19.5)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (0.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (1.34.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (1.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from TensorBoard) (3.3.4)\n",
      "Requirement already satisfied: six in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from absl-py>=0.4->TensorBoard) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->TensorBoard) (4.7.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->TensorBoard) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->TensorBoard) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->TensorBoard) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->TensorBoard) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from requests<3,>=2.21.0->TensorBoard) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from requests<3,>=2.21.0->TensorBoard) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from requests<3,>=2.21.0->TensorBoard) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from requests<3,>=2.21.0->TensorBoard) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /zfs1/hdaqing/kmt81/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->TensorBoard) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install dill\n",
    "!pip install imbalanced-learn\n",
    "!pip install TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherannotation = pd.read_csv('data/comments_opinion_anno.csv')\n",
    "add_opinoin=pd.Series(otherannotation.opinion.values, index=otherannotation.comments_id).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8a2dd9071a55>:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  remainingdata.dropna(subset=['comment_text'],inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['A/P', 'comments_id', 'comments_user', 'comment_text', 'advice',\n",
       "       'referral', 'fact or situation appraisal', 'personal Experience',\n",
       "       'opinion', 'emotional', 'others'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading csv data files using pandas dataframe \n",
    "alldata = pd.read_csv(\"data/file_with_post_annotated.csv\")\n",
    "# columns comment_text, comment_id, advice (other columns for annotation)\n",
    "train_size = 0.8\n",
    "train=alldata.sample(frac=train_size,random_state=200)\n",
    "test_pred=alldata.drop(train.index).reset_index(drop=True)\n",
    "train= train.reset_index(drop=True)\n",
    "df=train\n",
    "df=df.astype({\"advice\": int,'referral': int,\n",
    "       'fact or situation appraisal': int, 'personal Experience': int, 'opinion': int,\n",
    "       'emotional': int, 'others': int})\n",
    "train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncodedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds(df, tokenizer,column_name):\n",
    "    labels = df[column_name].values\n",
    "#     print(labels)\n",
    "    text = df[\"comment_text\"].tolist()\n",
    "    encodings = tokenizer(text, truncation=True, padding=True)\n",
    "    ds = TextEncodedDataset(encodings, labels)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(trainer, val_loader):\n",
    "    res = trainer.prediction_loop(val_loader, description=\"Preds\")\n",
    "    logits = res[0]\n",
    "    labels = res[1]\n",
    "    pred_probas = softmax(logits, axis=1)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, pred_probas)\n",
    "    auc_score = auc(recall, precision)\n",
    "    return {\"auc_score\": auc_score, \"pos_samples\": labels.sum()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling from  majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random under-sampling:\n",
      "0    425\n",
      "1    122\n",
      "Name: opinion, dtype: int64\n",
      "Counter({'opinion': 1})\n",
      "382 83 82\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "from collections import Counter\n",
    "X_unlabel = remainingdata.copy()\n",
    "X_unlabel=X_unlabel.astype({\"advice\": int,'referral': int,\n",
    "       'fact or situation appraisal': int, 'personal Experience': int, 'opinion': int,\n",
    "       'emotional': int, 'others': int})\n",
    "\n",
    "X=X.astype({\"advice\": int,'referral': int,\n",
    "       'fact or situation appraisal': int, 'personal Experience': int, 'opinion': int,\n",
    "       'emotional': int, 'others': int})\n",
    "column_name='opinion'\n",
    "print('Random under-sampling:')\n",
    "print(X[column_name].value_counts())\n",
    "y=X[column_name].to_frame()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,stratify=y, test_size=0.3)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "        X_test, y_test, stratify=y_test, test_size=0.5)\n",
    "\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "print(len(X_train),len(X_val),len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>advice</th>\n",
       "      <th>referral</th>\n",
       "      <th>fact or situation appraisal</th>\n",
       "      <th>personal Experience</th>\n",
       "      <th>opinion</th>\n",
       "      <th>emotional</th>\n",
       "      <th>others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>I AM WRITING AS A 22 YEAR SURVIVOR OF OV CA AN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>I am glad you are searching out help on this s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Hi Penny, I was diagnosed 3c in February 2013....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Its my understanding that a clinical trial eva...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Yes I would. I would like to know if any of yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Its very hard to know as we all respond differ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>i do taxoterene on monday, f-5U pump for 48 ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Recently I was treated with Taxol for 11 weeks...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>My sister has been diagnosed with Ovarian Canc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>Penny, I am a Stage IiC who went 5-1/2 years b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comment_text  advice  referral  \\\n",
       "816  I AM WRITING AS A 22 YEAR SURVIVOR OF OV CA AN...       0         0   \n",
       "183  I am glad you are searching out help on this s...       1         1   \n",
       "353  Hi Penny, I was diagnosed 3c in February 2013....       0         0   \n",
       "6    Its my understanding that a clinical trial eva...       0         0   \n",
       "361  Yes I would. I would like to know if any of yo...       0         0   \n",
       "..                                                 ...     ...       ...   \n",
       "795  Its very hard to know as we all respond differ...       1         1   \n",
       "257  i do taxoterene on monday, f-5U pump for 48 ho...       0         0   \n",
       "164  Recently I was treated with Taxol for 11 weeks...       1         0   \n",
       "62   My sister has been diagnosed with Ovarian Canc...       0         0   \n",
       "352  Penny, I am a Stage IiC who went 5-1/2 years b...       0         1   \n",
       "\n",
       "     fact or situation appraisal  personal Experience  opinion  emotional  \\\n",
       "816                            1                    1        1          0   \n",
       "183                            0                    1        0          1   \n",
       "353                            0                    1        0          0   \n",
       "6                              1                    0        1          0   \n",
       "361                            1                    1        0          1   \n",
       "..                           ...                  ...      ...        ...   \n",
       "795                            1                    0        1          0   \n",
       "257                            1                    1        0          1   \n",
       "164                            0                    1        0          1   \n",
       "62                             0                    1        1          0   \n",
       "352                            0                    1        0          1   \n",
       "\n",
       "     others  \n",
       "816       0  \n",
       "183       0  \n",
       "353       0  \n",
       "6         0  \n",
       "361       0  \n",
       "..      ...  \n",
       "795       0  \n",
       "257       0  \n",
       "164       0  \n",
       "62        0  \n",
       "352       0  \n",
       "\n",
       "[382 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122, 751, 122, 85, 19, 18]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "N = 6\n",
    "ind = np.arange(N) \n",
    "width = 0.25\n",
    "xvals=[]\n",
    "yvals=[]\n",
    "zvals=[]\n",
    "for df_bar in [alldata,X_unlabel,X,X_train,X_val,X_test]:\n",
    "    xvals.append(len(df_bar[df_bar[column_name] == 0]))\n",
    "    yvals.append(len(df_bar[df_bar[column_name] == 1]))\n",
    "\n",
    "bar1 = plt.bar(ind+width, xvals, width, color = 'r')\n",
    "bar2 = plt.bar(ind, yvals, width, color='g')\n",
    "\n",
    "print(yvals)\n",
    "  \n",
    "plt.xticks(ind+width,['All Data','Unlabeled Data', 'Downsample Data','Training','Validation','Test'],rotation=45)\n",
    "plt.legend( (bar1, bar2), ('0', '1') )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n",
      "Now running 382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b686e19b1838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m#tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     trainer = Trainer(\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# the instantiated 🤗 Transformers model to be trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# training arguments, defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# Model parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_model_parallel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/anaconda3/envs/jupyterkernel/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for column_name in ['opinion']:\n",
    "    print(len(X_train))\n",
    "    val_ds = create_ds(X_val, tokenizer,column_name)\n",
    "    val_loader = DataLoader(val_ds, batch_size=16, shuffle=True)\n",
    "\n",
    "    pred_ds = create_ds(X_test,tokenizer,column_name)\n",
    "    pred_loader = DataLoader(pred_ds, batch_size=16, shuffle=False)\n",
    "    # n_sample_list = [ len(train_filt_full) ]#, 2000, 10000, 50000] #, 100000]\n",
    "\n",
    "    train_dataset = create_ds(X_train, tokenizer,column_name)\n",
    "    val_dataset = create_ds(X_val, tokenizer,column_name)\n",
    "    pred_dataset =create_ds(X_test, tokenizer,column_name)\n",
    "\n",
    "\n",
    "    n_samples = len(X_train)\n",
    "    d_res = dict()\n",
    "    #for n_samples in n_sample_list:\n",
    "    print(\"Now running\", n_samples)\n",
    "    # train_filt = train_filt_full.sample(n_samples, random_state=0)\n",
    "    # train_filt, test = train_test_split(train_filt, test_size=0.2, random_state=0)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./output_anno/output_inc//results_role/'+column_name.replace(\" \",\"_\"),          # output directory\n",
    "        num_train_epochs=15,              # total number of training epochs\n",
    "        per_device_train_batch_size=8,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "        warmup_steps=50,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./output_anno/output_inc//logs_role/'+column_name.replace(\" \",\"_\"),            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        do_eval=True,\n",
    "        do_predict=True,\n",
    "        eval_steps=10,\n",
    "        evaluation_strategy='steps',\n",
    "        load_best_model_at_end=True,\n",
    "\n",
    "    )\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "    #logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=val_dataset   ,         # evaluation dataset\n",
    "        compute_metrics=compute_metrics,\n",
    "       #callbacks=[tensorboard_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    d_res[n_samples] = evaluate(trainer, val_loader)\n",
    "    pred_test = list(X_test['comment_text'])\n",
    "    pred_labels = list(y_test[column_name].astype(int))\n",
    "\n",
    "    encodings = tokenizer(pred_test, truncation=True, padding=True)\n",
    "    ds = TextEncodedDataset(encodings, pred_labels)\n",
    "    res = trainer.predict(ds)\n",
    "    # pred_dataset = create_ds(test_pred, tokenizer,column_name)\n",
    "    # res = trainer.predict(pred_dataset)\n",
    "    output=torch.argmax(torch.from_numpy(softmax(res.predictions, axis=1)),axis=1)\n",
    "    score=f1_score(output,pred_labels)\n",
    "    print(column_name,\":\")\n",
    "    print(\"f1-score\",score)\n",
    "    print(\"precision\",precision_score(output,pred_labels))\n",
    "    print(\"recall\",recall_score(output,pred_labels))\n",
    "    print(\"accuracy\", accuracy_score(output,pred_labels))\n",
    "    print()\n",
    "    # plot_confusion_matrix(clf, X_test, y_test) \n",
    "    plt.show()\n",
    "    trainer.save_model('./output_anno/output_inc//results_role/'+column_name.replace(\" \",\"_\")+'/' + 'final_model.pt')\n",
    "\n",
    "    with open(\"./output_anno/output_inc//results_role/\"+column_name.replace(\" \",\"_\")+\"/output_model_final.dill\", \"wb\") as dill_file:\n",
    "        dill.dump(trainer, dill_file)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-86bbbae6ce85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix( output,pred_labels) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-889dcb36370c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextEncodedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_labels_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# pred_dataset = create_ds(test_pred, tokenizer,column_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "pred_test_2 = list(remainingdata['comment_text'])\n",
    "pred_labels_2 = list(remainingdata[column_name].astype(int))\n",
    "\n",
    "encodings = tokenizer(pred_test_2, truncation=True, padding=True)\n",
    "ds = TextEncodedDataset(encodings, pred_labels_2)\n",
    "res = trainer.predict(ds)\n",
    "\n",
    "# pred_dataset = create_ds(test_pred, tokenizer,column_name)\n",
    "# res = trainer.predict(pred_dataset)\n",
    "output=torch.argmax(torch.from_numpy(softmax(res.predictions, axis=1)),axis=1)\n",
    "remainingdata['advice_new']=output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opinion :\n",
      "f1-score 0.6520065970313358\n",
      "precision 0.7896138482023968\n",
      "recall 0.5552434456928839\n",
      "accuracy 0.8162020905923345\n"
     ]
    }
   ],
   "source": [
    "score=f1_score(output,pred_labels_2)\n",
    "print(column_name,\":\")\n",
    "print(\"f1-score\",score)\n",
    "print(\"precision\",precision_score(output,pred_labels_2))\n",
    "print(\"recall\",recall_score(output,pred_labels_2))\n",
    "print(\"accuracy\", accuracy_score(output,pred_labels_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(input_text,model):\n",
    "    inputs = tokenizer(input_text,truncation=True, padding=True,return_tensors=\"pt\")\n",
    "#     inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "    \n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    classp = np.argmax(softmax(logits.detach().numpy()))\n",
    "    return classp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = {}\n",
    "\n",
    "for key in \n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    './output_anno/final_models//'+column_name.replace(\" \",\"_\")+'/' + 'final_model.pt'    )\n",
    "model.eval()\n",
    "\n",
    "model_advice = DistilBertForSequenceClassification.from_pretrained(\n",
    "    './output_anno//final_models/'+column_name.replace(\" \",\"_\")+'/' + 'final_model.pt'    )\n",
    "model_advice.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fact_or_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_infer_opinion(input_text,model,model_advice):\n",
    "    infer_list={}\n",
    "    infer_list[0]=[]\n",
    "    infer_list[1]=[]\n",
    "    no_sent=True\n",
    "    classp=0\n",
    "    input_text = input_text.replace(\"\\n\",\" \")\n",
    "    for sent in list(nlp(str(input_text)).sents):\n",
    "        if len(str(sent).strip().split(\" \")) > 3:\n",
    "            no_sent=False\n",
    "            \n",
    "            # advice \n",
    "            inputs_advice = tokenizer(str(sent),truncation=True, padding=True,return_tensors=\"pt\")\n",
    "            labels_advice = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "            outputs_advice = model_advice(**inputs_advice, labels=labels_advice)\n",
    "            loss_advice = outputs_advice.loss\n",
    "            logits_advice = outputs_advice.logits\n",
    "            probs_advice=softmax(logits_advice.detach().numpy())\n",
    "    #         print(probs)\n",
    "            classp_advice = np.argmax(probs_advice)\n",
    "            \n",
    "            if classp_advice == 1:\n",
    "                infer_list[classp].append(0)\n",
    "                classp=0\n",
    "                continue\n",
    "            inputs = tokenizer(str(sent),truncation=True, padding=True,return_tensors=\"pt\")\n",
    "            labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            probs=softmax(logits.detach().numpy())\n",
    "    #         print(probs)\n",
    "            classp = np.argmax(probs)\n",
    "            infer_list[classp].append(probs[0][classp])\n",
    "#             print(str(sent),classp)\n",
    "    classp=0\n",
    "    if len(infer_list[1]) > 0:\n",
    "        if max(infer_list[1]) > 0.60:\n",
    "            classp=1\n",
    "        else:\n",
    "            classp = 0\n",
    "        \n",
    "    if no_sent==True:\n",
    "        infer_list[0].append(1)\n",
    "#     print(infer_list)\n",
    "    return classp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_infer(input_text,model):\n",
    "    infer_list={}\n",
    "    infer_list[0]=[]\n",
    "    infer_list[1]=[]\n",
    "    no_sent=True\n",
    "    classp=0\n",
    "    input_text = input_text.replace(\"\\n\",\" \")\n",
    "    for sent in list(nlp(str(input_text)).sents):\n",
    "        if len(str(sent).strip().split(\" \")) > 3:\n",
    "            no_sent=False\n",
    "            \n",
    "            # advice \n",
    "            inputs_advice = tokenizer(str(sent),truncation=True, padding=True,return_tensors=\"pt\")\n",
    "            labels_advice = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "            outputs_advice = model_advice(**inputs_advice, labels=labels_advice)\n",
    "            loss_advice = outputs_advice.loss\n",
    "            logits_advice = outputs_advice.logits\n",
    "            probs_advice=softmax(logits_advice.detach().numpy())\n",
    "    #         print(probs)\n",
    "            classp_advice = np.argmax(probs_advice)\n",
    "            \n",
    "            if classp_advice == 1:\n",
    "                infer_list[classp].append(0)\n",
    "                classp=0\n",
    "                continue\n",
    "            inputs = tokenizer(str(sent),truncation=True, padding=True,return_tensors=\"pt\")\n",
    "            labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            probs=softmax(logits.detach().numpy())\n",
    "    #         print(probs)\n",
    "            classp = np.argmax(probs)\n",
    "            infer_list[classp].append(probs[0][classp])\n",
    "#             print(str(sent),classp)\n",
    "    classp=0\n",
    "    if len(infer_list[1]) > 0:\n",
    "        if max(infer_list[1]) > 0.50:\n",
    "            classp=1\n",
    "        else:\n",
    "            classp = 0\n",
    "        \n",
    "    if no_sent==True:\n",
    "        infer_list[0].append(1)\n",
    "#     print(infer_list)\n",
    "    return classp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(input_text,model):\n",
    "    inputs = tokenizer(input_text,truncation=True, padding=True,return_tensors=\"pt\")\n",
    "#     inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "    \n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    classp = np.argmax(softmax(logits.detach().numpy()))\n",
    "    return classp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()\n",
    "for index,row in X_test.iterrows():\n",
    "#     print()\n",
    "#     print(\"--- start ----\")\n",
    "#     print(\"comment_text\",row['comment_text'])\n",
    "#     print()\n",
    "#     print(\"-- sentence wise result\")\n",
    "    pred=sent_infer(row['comment_text'],model,model_advice)\n",
    "    output.append(pred)\n",
    "#     print(\"-------------\")\n",
    "#     print(\"##real - prediction - \",row['opinion'],pred)\n",
    "#     print(\"-------------\")\n",
    "#     print(\"-------end----------\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opinion :\n",
      "f1-score 0.8076923076923077\n",
      "precision 0.8076923076923077\n",
      "recall 0.8076923076923077\n",
      "accuracy 0.8780487804878049\n",
      "confusion [[51  5]\n",
      " [ 5 21]]\n"
     ]
    }
   ],
   "source": [
    "# output=X_test[column_name+\"_pred_sent_output\"  ].fillna(1)\n",
    "pred_labels = list(X_test[column_name].astype(int))\n",
    "\n",
    "score=f1_score(output,pred_labels)\n",
    "print(column_name,\":\")\n",
    "print(\"f1-score\",score)\n",
    "print(\"precision\",precision_score(output,pred_labels))\n",
    "print(\"recall\",recall_score(output,pred_labels))\n",
    "print(\"accuracy\", accuracy_score(output,pred_labels))\n",
    "print(\"confusion\",confusion_matrix(output,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterkernel",
   "language": "python",
   "name": "jupyterkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}